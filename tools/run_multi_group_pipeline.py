#!/usr/bin/env python3
"""
å¤šåˆ†ç»„éª¨æ¶æµæ°´çº¿ - GIFåŠ¨ç”»ç”Ÿæˆå™¨
å®Œå…¨å€Ÿé‰´skeleton_extraction_reconstruction_pipeline.pyçš„å¯è§†åŒ–æ–¹æ³•
ç”Ÿæˆè¿ç»­å¸§åºåˆ—çš„GIFåŠ¨ç”»

éª¨æ¶æå–å™¨: OptimizedMARSModel (æç®€ä¼˜åŒ–ç‰ˆ)
- æ¨¡å‹è·¯å¾„: models/skeleton_extractor_final.py
- é»˜è®¤æƒé‡: mars_optimized_best.pth
- æ¶æ„ç‰¹ç‚¹: SpatialPreservingBackbone + SimplifiedRegressionHead
- æŸå¤±å‡½æ•°: MSE(0.7) + L1(0.3), æ— å¤æ‚çº¦æŸ
"""

import os
import sys
import numpy as np
import torch
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
from mpl_toolkits.mplot3d import Axes3D
import json
import argparse

# è®¾ç½®matplotlib
import matplotlib
matplotlib.use('Agg')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.multi_group_skeleton_pipeline import create_pipeline, GROUPING_CONFIGS

# MARSæ•°æ®é›†æ˜ å°„å…³èŠ‚å¸¸é‡
FINGER_JOINTS = {7, 21, 22, 11, 23, 24}  # å·¦æ‰‹æŒ‡: 7,21,22  å³æ‰‹æŒ‡: 11,23,24

# NTUéª¨æ¶è¿æ¥å…³ç³»
SKELETON_EDGES = [
    (3, 2), (2, 20), (20, 1), (1, 0),  # å¤´éƒ¨å’Œè„ŠæŸ±
    (20, 4), (4, 5), (5, 6),           # å·¦è‡‚
    (6, 22), (6, 7), (7, 21),          # å·¦æ‰‹æŒ‡(æ˜ å°„)
    (20, 8), (8, 9), (9, 10),          # å³è‡‚
    (10, 24), (10, 11), (11, 23),      # å³æ‰‹æŒ‡(æ˜ å°„)
    (0, 12), (12, 13), (13, 14), (14, 15),  # å·¦è…¿
    (0, 16), (16, 17), (17, 18), (18, 19)   # å³è…¿
]


def calculate_mse_without_fingers(skeleton1, skeleton2):
    """è®¡ç®—MSEï¼Œæ’é™¤æ‰‹æŒ‡6å…³èŠ‚çš„å½±å“"""
    if isinstance(skeleton1, torch.Tensor):
        skeleton1 = skeleton1.cpu().numpy()
    if isinstance(skeleton2, torch.Tensor):
        skeleton2 = skeleton2.cpu().numpy()
    
    # åˆ›å»ºæ©ç ï¼šä»…åŒ…å«éæ‰‹æŒ‡å…³èŠ‚
    mask = np.ones(skeleton1.shape[0], dtype=bool)
    for joint_idx in FINGER_JOINTS:
        mask[joint_idx] = False
    
    # ä»…è®¡ç®—19ä¸ªçœŸå®å…³èŠ‚çš„MSE
    return np.mean((skeleton1[mask] - skeleton2[mask])**2)


def plot_skeleton_3d(ax, skeleton, title, color='blue'):
    """ç»˜åˆ¶3Déª¨æ¶(ä¸æ˜¾ç¤ºæ‰‹æŒ‡å…³èŠ‚)"""
    if isinstance(skeleton, torch.Tensor):
        skeleton = skeleton.cpu().numpy()
    
    skeleton = skeleton.copy()
    skeleton[:, 2] = -skeleton[:, 2]  # åè½¬Zè½´æ”¹å–„è§†è§’
    
    # è¿‡æ»¤æ‰‹æŒ‡å…³èŠ‚çš„è¾¹
    edges = [e for e in SKELETON_EDGES if e[0] not in FINGER_JOINTS and e[1] not in FINGER_JOINTS]
    
    # ç»˜åˆ¶éª¨éª¼è¿æ¥
    for start_idx, end_idx in edges:
        start, end = skeleton[start_idx], skeleton[end_idx]
        if not (np.all(start == 0) or np.all(end == 0)):
            ax.plot3D([start[0], end[0]], [start[1], end[1]], [start[2], end[2]],
                     color=color, alpha=0.8, linewidth=2.0)
    
    # ç»˜åˆ¶å…³èŠ‚ç‚¹(æ’é™¤æ‰‹æŒ‡)
    for i in range(len(skeleton)):
        if i in FINGER_JOINTS:
            continue
        joint = skeleton[i]
        if not np.all(joint == 0):
            ax.scatter(joint[0], joint[1], joint[2],
                      c=color, s=25, alpha=0.9, edgecolors='white', linewidth=0.5)
    
    ax.set_title(title, fontsize=12, fontweight='bold')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    
    # è®¾ç½®ç­‰æ¯”ä¾‹åæ ‡èŒƒå›´
    valid_joints = skeleton[~np.all(skeleton == 0, axis=1)]
    if len(valid_joints) > 0:
        center = np.mean(valid_joints, axis=0)
        max_range = max(np.max(np.max(valid_joints, axis=0) - np.min(valid_joints, axis=0)) / 2, 0.3)
        
        ax.set_xlim([center[0] - max_range, center[0] + max_range])
        ax.set_ylim([center[1] - max_range, center[1] + max_range])
        ax.set_zlim([center[2] - max_range, center[2] + max_range])
        ax.set_box_aspect([1, 1, 1])
    
    ax.view_init(elev=15, azim=45)


def apply_temporal_smoothing(data, window_size=5):
    """
    å¯¹æ•°æ®è¿›è¡Œæ—¶åºå¹³æ»‘ (ç§»åŠ¨å¹³å‡)
    data: [T, ...] numpy array
    """
    if window_size <= 1:
        return data
    
    T = data.shape[0]
    if T < window_size:
        return data
        
    # print(f"   ğŸ”„ åº”ç”¨æ—¶åºå¹³æ»‘ (Window={window_size})...")
    
    # Reshape to [T, -1] for easy processing
    original_shape = data.shape
    flattened = data.reshape(T, -1)
    smoothed_flat = np.zeros_like(flattened)
    pad_size = window_size // 2
    
    for i in range(flattened.shape[1]):
        padded = np.pad(flattened[:, i], (pad_size, pad_size), mode='edge')
        kernel = np.ones(window_size) / window_size
        convolved = np.convolve(padded, kernel, mode='valid')
        
        if len(convolved) > T:
            convolved = convolved[:T]
        smoothed_flat[:, i] = convolved
        
    return smoothed_flat.reshape(original_shape)


def generate_gif_animation(pipeline, radar_data_path, output_dir, grouping_type, 
                           num_sequences=10, frames_per_sequence=8, fps=3):
    """ç”ŸæˆGIFåŠ¨ç”»"""
    print(f"\nğŸ¬ ç”Ÿæˆ {grouping_type.upper()} GIF...")
    
    gif_output_dir = os.path.join(output_dir, f"gif_10p_adaptive_576_balance" if grouping_type == '10p' else f"gif_{grouping_type}")
    os.makedirs(gif_output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    labels_path = '/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy'
    if not os.path.exists(radar_data_path) or not os.path.exists(labels_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return []
    
    full_data = np.load(radar_data_path)
    label_data = np.load(labels_path)
    print(f"âœ… æ•°æ®: radar {full_data.shape}, label {label_data.shape}")
    
    gif_info_list = []
    
    # ç”Ÿæˆå¤šä¸ªåºåˆ—çš„GIF
    for seq_idx in range(num_sequences):
        # ä¸ºæ¯ä¸ªåºåˆ—é€‰æ‹©ä¸åŒçš„èµ·å§‹ä½ç½®
        start_idx = seq_idx * (len(full_data) // (num_sequences + 1))
        end_idx = min(start_idx + frames_per_sequence, len(full_data))
        
        if end_idx - start_idx < frames_per_sequence:
            # å¦‚æœæ•°æ®ä¸å¤Ÿï¼Œä»æœ«å°¾å‘å‰å–
            end_idx = len(full_data) - 1
            start_idx = max(0, end_idx - frames_per_sequence + 1)
        
        print(f"ğŸ“¹ ç”Ÿæˆåºåˆ— {seq_idx+1}/{num_sequences}: å¸§ {start_idx}-{end_idx-1}")
        
        # æå–åºåˆ—æ•°æ®
        sequence_data = full_data[start_idx:end_idx]
        sequence_labels = label_data[start_idx:end_idx]
        
        # 1. æ‰¹é‡æå–éª¨æ¶
        extracted_skeletons = []
        for radar_frame in sequence_data:
            radar_tensor = torch.from_numpy(radar_frame.transpose(2, 0, 1)).unsqueeze(0).float().to(pipeline.device)
            extracted = pipeline.extract_skeleton(radar_tensor)
            extracted_skeletons.append(extracted.cpu().numpy())
            
        extracted_skeletons = np.concatenate(extracted_skeletons, axis=0) # [T, 25, 3]
        
        # 2. åº”ç”¨æ—¶åºå¹³æ»‘
        smoothed_skeletons = apply_temporal_smoothing(extracted_skeletons, window_size=5)
        
        # 3. é‡æ„ä¸ç»“æœæ”¶é›†
        frame_results = []
        for frame_idx, (label_frame, smoothed_skel) in enumerate(zip(sequence_labels, smoothed_skeletons)):
            # ä½¿ç”¨å¹³æ»‘åçš„éª¨æ¶è¿›è¡Œé‡æ„
            smoothed_skel_tensor = torch.from_numpy(smoothed_skel).unsqueeze(0).to(pipeline.device)
            
            # é‡æ„
            recon_result = pipeline.reconstruct_skeleton(smoothed_skel_tensor)
            
            # è®¡ç®—MSE(æ’é™¤æ‰‹æŒ‡6å…³èŠ‚)
            mse = calculate_mse_without_fingers(
                smoothed_skel,
                recon_result['reconstructed'][0].cpu().numpy()
            )
            
            frame_results.append({
                'frame_idx': frame_idx,
                'label': label_frame,
                'extracted': smoothed_skel,
                'reconstructed': recon_result['reconstructed'][0].cpu().numpy(),
                'mse': mse
            })
        
        # ç”ŸæˆGIF
        gif_path = os.path.join(gif_output_dir, f'skeleton_reconstruction_sequence_{seq_idx+1:02d}.gif')
        gif_info = create_skeleton_gif(frame_results, gif_path, grouping_type, pipeline, fps)
        gif_info.update({'sequence_id': seq_idx + 1, 'start_frame': start_idx, 'end_frame': end_idx - 1})
        gif_info_list.append(gif_info)
    
    return gif_info_list


def create_skeleton_gif(frame_results, gif_path, grouping_type, pipeline, fps=3):
    """åˆ›å»ºéª¨æ¶é‡æ„GIFåŠ¨ç”»"""
    num_frames = len(frame_results)
    if num_frames == 0:
        return {'success': False, 'path': gif_path}
    
    fig = plt.figure(figsize=(24, 8))
    plt.rcParams.update({
        'font.sans-serif': ['DejaVu Sans'],
        'axes.unicode_minus': False,
        'font.size': 10
    })
    
    def animate(frame_idx):
        fig.clear()
        current = frame_results[frame_idx]
        
        # è½¬æ¢MARSæ ‡ç­¾ä¸ºNTUæ ¼å¼
        mars_tensor = torch.tensor(current['label']).unsqueeze(0).float().to(pipeline.device)
        label_ntu = pipeline.joint_mapper(mars_tensor)[0].detach().cpu().numpy()
        
        # åˆ›å»º3ä¸ªå­å›¾
        ax1 = fig.add_subplot(131, projection='3d')
        ax2 = fig.add_subplot(132, projection='3d')
        ax3 = fig.add_subplot(133, projection='3d')
        
        # ç»˜åˆ¶ä¸‰ç§éª¨æ¶(å‡ä¸æ˜¾ç¤ºæ‰‹æŒ‡)
        plot_skeleton_3d(ax1, label_ntu, f'Frame {frame_idx+1}/{num_frames}: Ground Truth', 'blue')
        plot_skeleton_3d(ax2, current['extracted'], f'Frame {frame_idx+1}/{num_frames}: Extracted', 'green')
        plot_skeleton_3d(ax3, current['reconstructed'], f'Frame {frame_idx+1}/{num_frames}: Reconstructed', 'red')
        
        # æ ‡é¢˜(MSEå·²æ’é™¤æ‰‹æŒ‡å…³èŠ‚)
        fig.suptitle(f'Skeleton Reconstruction | Frame {frame_idx+1}/{num_frames} | MSE (19 joints): {current["mse"]:.6f}',
                    fontsize=14, fontweight='bold', y=0.95)
        plt.tight_layout()
    
    try:
        anim = FuncAnimation(fig, animate, frames=num_frames, interval=1000//fps, blit=False, repeat=True)
        anim.save(gif_path, writer=PillowWriter(fps=fps), dpi=150)
        plt.close(fig)
        
        print(f"âœ… GIFä¿å­˜: {os.path.basename(gif_path)}")
        
        mse_errors = [fr['mse'] for fr in frame_results]
        return {
            'success': True,
            'path': gif_path,
            'num_frames': num_frames,
            'avg_mse': float(np.mean(mse_errors)),
            'min_mse': float(np.min(mse_errors)),
            'max_mse': float(np.max(mse_errors)),
            'frame_range': (0, num_frames-1)
        }
    except Exception as e:
        print(f"âŒ GIFç”Ÿæˆå¤±è´¥: {e}")
        plt.close(fig)
        return {'success': False, 'path': gif_path, 'error': str(e)}


def create_random_png_snapshots(pipeline, radar_data_path, output_dir, grouping_type, num_snapshots=20):
    """
    ç”Ÿæˆéšæœº20å¸§PNGå¿«ç…§ï¼Œå±•ç¤ºGTã€æå–éª¨æ¶å’Œé‡æ„éª¨æ¶çš„å¯¹æ¯”
    """
    print(f"\nğŸ“¸ å¼€å§‹ç”Ÿæˆ {num_snapshots} ä¸ªéšæœºPNGå¿«ç…§...")
    
    # åˆ›å»ºPNGè¾“å‡ºç›®å½•
    png_dir = os.path.join(output_dir, f"png_snapshots_10p_adaptive_576_balance" if grouping_type == '10p' else f"png_snapshots_{grouping_type}")
    os.makedirs(png_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    labels_path = '/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy'
    if not os.path.exists(radar_data_path) or not os.path.exists(labels_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return None
    
    full_data = np.load(radar_data_path)
    label_data = np.load(labels_path)
    total_samples = len(full_data)
    
    # éšæœºé€‰æ‹©ä¸é‡å¤çš„å¸§ç´¢å¼•
    np.random.seed(42)
    random_indices = np.random.choice(total_samples, size=min(num_snapshots, total_samples), replace=False)
    random_indices = sorted(random_indices.tolist())
    
    print(f"âœ“ éšæœºé€‰æ‹©çš„å¸§ç´¢å¼•: {random_indices[:10]}..." if len(random_indices) > 10 else f"âœ“ éšæœºé€‰æ‹©çš„å¸§ç´¢å¼•: {random_indices}")
    
    snapshot_results = []
    
    for idx, sample_idx in enumerate(random_indices, 1):
        # åŠ è½½æ•°æ®
        radar_frame = full_data[sample_idx]
        label_frame = label_data[sample_idx]
        
        # å¤„ç†
        radar_tensor = torch.from_numpy(radar_frame.transpose(2, 0, 1)).unsqueeze(0).float().to(pipeline.device)
        result = pipeline.process_full_pipeline(radar_tensor)
        
        # è½¬æ¢MARSæ ‡ç­¾ä¸ºNTUæ ¼å¼
        mars_tensor = torch.tensor(label_frame).unsqueeze(0).float().to(pipeline.device)
        label_ntu = pipeline.joint_mapper(mars_tensor)[0].detach().cpu().numpy()
        
        extracted = result['extracted'][0].cpu().numpy()
        reconstructed = result['reconstructed'][0].cpu().numpy()
        
        # è®¡ç®—MSE(æ’é™¤æ‰‹æŒ‡)
        mse_extracted = calculate_mse_without_fingers(label_ntu, extracted)
        mse_reconstructed = calculate_mse_without_fingers(label_ntu, reconstructed)
        
        # åˆ›å»ºå›¾å½¢
        fig = plt.figure(figsize=(24, 8))
        plt.rcParams.update({
            'font.sans-serif': ['DejaVu Sans'],
            'axes.unicode_minus': False,
            'font.size': 10
        })
        
        # 3ä¸ªå­å›¾
        ax1 = fig.add_subplot(131, projection='3d')
        ax2 = fig.add_subplot(132, projection='3d')
        ax3 = fig.add_subplot(133, projection='3d')
        
        # ç»˜åˆ¶ä¸‰ç§éª¨æ¶(å‡ä¸æ˜¾ç¤ºæ‰‹æŒ‡)
        plot_skeleton_3d(ax1, label_ntu, f'Frame {sample_idx+1}: Ground Truth', 'blue')
        plot_skeleton_3d(ax2, extracted, f'Frame {sample_idx+1}: Extracted', 'green')
        plot_skeleton_3d(ax3, reconstructed, f'Frame {sample_idx+1}: Reconstructed', 'red')
        
        # æ ‡é¢˜
        title_text = (f'Skeleton Pipeline ({grouping_type.upper()}) | Frame {sample_idx+1:04d} | '
                     f'MSE Extracted: {mse_extracted:.6f} | MSE Reconstructed: {mse_reconstructed:.6f} (19 joints)')
        fig.suptitle(title_text, fontsize=14, fontweight='bold', y=0.95)
        
        # ä¿å­˜PNG
        png_path = os.path.join(png_dir, f'skeleton_frame_{sample_idx+1:04d}.png')
        plt.tight_layout()
        plt.savefig(png_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        snapshot_results.append({
            'frame_idx': sample_idx,
            'mse_extracted': float(mse_extracted),
            'mse_reconstructed': float(mse_reconstructed)
        })
        
        if idx % 5 == 0 or idx == len(random_indices):
            print(f"  è¿›åº¦: {idx}/{len(random_indices)} å¼ PNGå·²ç”Ÿæˆ")
    
    print(f"âœ… PNGå¿«ç…§ç”Ÿæˆå®Œæˆ!")
    print(f"è¾“å‡ºç›®å½•: {png_dir}/")
    
    # ç»Ÿè®¡ä¿¡æ¯
    mse_extracted_list = [r['mse_extracted'] for r in snapshot_results]
    mse_reconstructed_list = [r['mse_reconstructed'] for r in snapshot_results]
    
    stats = {
        'num_snapshots': len(snapshot_results),
        'sample_indices': random_indices,
        'avg_mse_extracted': float(np.mean(mse_extracted_list)),
        'avg_mse_reconstructed': float(np.mean(mse_reconstructed_list)),
        'min_mse_extracted': float(np.min(mse_extracted_list)),
        'min_mse_reconstructed': float(np.min(mse_reconstructed_list)),
        'max_mse_extracted': float(np.max(mse_extracted_list)),
        'max_mse_reconstructed': float(np.max(mse_reconstructed_list)),
        'output_dir': png_dir
    }
    
    # ä¿å­˜ç»Ÿè®¡
    stats_path = os.path.join(png_dir, 'png_statistics.json')
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ“ PNGç»Ÿè®¡: æå–MSE {stats['avg_mse_extracted']:.6f}, é‡æ„MSE {stats['avg_mse_reconstructed']:.6f}")
    
    return stats


def process_with_grouping_gif(grouping_type, radar_data_path, output_base_dir, generate_png=True):
    """ä½¿ç”¨æŒ‡å®šåˆ†ç»„ç±»å‹å¤„ç†æ•°æ®å¹¶ç”ŸæˆGIF"""
    print("\n" + "=" * 80)
    print(f"å¤„ç† {grouping_type.upper()} åˆ†ç»„")
    print("=" * 80)
    
    pipeline = create_pipeline(grouping_type=grouping_type)
    pipeline.print_info()
    
    num_sequences = 10 if grouping_type == '10p' else 5

    gif_info_list = generate_gif_animation(
        pipeline=pipeline,
        radar_data_path=radar_data_path,
        output_dir=output_base_dir,
        grouping_type=grouping_type,
        num_sequences=num_sequences,
        frames_per_sequence=8,
        fps=2
    )
    
    output_dir = os.path.join(output_base_dir, f"gif_10p_adaptive_576_balance" if grouping_type == '10p' else f"gif_{grouping_type}")
    
    # ç”ŸæˆPNGå¿«ç…§
    png_stats = None
    if generate_png:
        print("\n" + "=" * 80)
        png_stats = create_random_png_snapshots(
            pipeline=pipeline,
            radar_data_path=radar_data_path,
            output_dir=output_base_dir,
            grouping_type=grouping_type,
            num_snapshots=20
        )
    
    stats = {
        'grouping_type': grouping_type,
        'grouping_name': GROUPING_CONFIGS[grouping_type].name,
        'num_sequences': len(gif_info_list),
        'note': 'MSEè®¡ç®—å·²æ’é™¤6ä¸ªæ‰‹æŒ‡æ˜ å°„å…³èŠ‚ï¼Œä»…åŸºäº19ä¸ªçœŸå®å…³èŠ‚',
        'png_snapshots': png_stats if png_stats else None,
        'sequences': [{
            'sequence_id': g['sequence_id'],
            'start_frame': g['start_frame'],
            'end_frame': g['end_frame'],
            'num_frames': g['num_frames'],
            'avg_mse': g['avg_mse'],
            'min_mse': g['min_mse'],
            'max_mse': g['max_mse'],
            'file': os.path.basename(g['path'])
        } for g in gif_info_list if g['success']]
    }
    
    with open(os.path.join(output_dir, 'gif_statistics.json'), 'w') as f:
        json.dump(stats, f, indent=2)
    
    # æ‰“å°ç»Ÿè®¡
    successful = sum(1 for g in gif_info_list if g['success'])
    print(f"\nğŸ“Š ç»Ÿè®¡: {successful}/{len(gif_info_list)} ä¸ªGIFæˆåŠŸ")
    
    if successful > 0:
        all_mse = [g['avg_mse'] for g in gif_info_list if g['success']]
        print(f"  GIF MSE (19å…³èŠ‚): {np.mean(all_mse):.6f} [{np.min(all_mse):.6f} - {np.max(all_mse):.6f}]")
    
    if png_stats:
        print(f"  PNG MSE æå–: {png_stats['avg_mse_extracted']:.6f} [{png_stats['min_mse_extracted']:.6f} - {png_stats['max_mse_extracted']:.6f}]")
        print(f"  PNG MSE é‡æ„: {png_stats['avg_mse_reconstructed']:.6f} [{png_stats['min_mse_reconstructed']:.6f} - {png_stats['max_mse_reconstructed']:.6f}]")
    
    print(f"\nğŸ“ è¾“å‡º: {output_dir}/")
    for g in gif_info_list:
        if g['success']:
            print(f"  âœ“ åºåˆ—{g['sequence_id']:02d}: {os.path.basename(g['path'])} (MSE: {g['avg_mse']:.6f})")
        else:
            print(f"  âœ— åºåˆ—{g['sequence_id']:02d}: {g.get('error', 'å¤±è´¥')}")


def compare_groupings_gif(radar_data_path, output_base_dir, generate_png=True):
    """å¯¹æ¯”ä¸åŒåˆ†ç»„é…ç½®çš„GIFæ•ˆæœ"""
    print("\n" + "=" * 80)
    print("å¯¹æ¯”æ¨¡å¼ - ç”Ÿæˆæ‰€æœ‰åˆ†ç»„GIF")
    print("=" * 80)
    
    for grouping_type in GROUPING_CONFIGS.keys():
        process_with_grouping_gif(grouping_type, radar_data_path, output_base_dir, generate_png=generate_png)
    
    comparison_stats = {
        'groupings': list(GROUPING_CONFIGS.keys()),
        'note': 'MSEåŸºäº19ä¸ªçœŸå®å…³èŠ‚(æ’é™¤6ä¸ªæ‰‹æŒ‡æ˜ å°„å…³èŠ‚)'
    }
    
    with open(os.path.join(output_base_dir, 'grouping_comparison_gif.json'), 'w') as f:
        json.dump(comparison_stats, f, indent=2)
    
    print("\nğŸ‰ æ‰€æœ‰åˆ†ç»„GIFç”Ÿæˆå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='å¤šåˆ†ç»„éª¨æ¶æµæ°´çº¿ - GIFåŠ¨ç”»ç”Ÿæˆå™¨')
    parser.add_argument('--mode', type=str, default='5p', 
                       choices=['5p', '8p', '10p', 'all', 'compare'],
                       help='å¤„ç†æ¨¡å¼: 5p=5åˆ†ç»„, 8p=8åˆ†ç»„, 10p=10åˆ†ç»„, all=æ‰€æœ‰åˆ†ç»„, compare=å¯¹æ¯”')
    parser.add_argument('--data', type=str, 
                       default='/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_test.npy',
                       help='é›·è¾¾æ•°æ®è·¯å¾„')
    parser.add_argument('--output', type=str, default='visualizations',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-png', action='store_true',
                       help='ç¦ç”¨PNGå¿«ç…§ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    generate_png = not args.no_png
    
    print("=" * 80)
    print("ğŸš€ å¤šåˆ†ç»„éª¨æ¶æµæ°´çº¿ - GIFåŠ¨ç”»ç”Ÿæˆå™¨")
    print("=" * 80)
    print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {args.data}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
    print(f"ğŸ¯ å¤„ç†æ¨¡å¼: {args.mode}")
    print(f"ğŸ“¸ PNGå¿«ç…§: {'å¯ç”¨' if generate_png else 'ç¦ç”¨'}")
    
    if args.mode == 'compare' or args.mode == 'all':
        compare_groupings_gif(args.data, args.output, generate_png=generate_png)
    else:
        process_with_grouping_gif(args.mode, args.data, args.output, generate_png=generate_png)


if __name__ == "__main__":
    main()
