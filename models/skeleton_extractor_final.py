#!/usr/bin/env python3
"""
MARSéª¨æ¶æå–ç®€åŒ–ç¨³å®šç‰ˆ (skeleton_extractor_final.py)
==========================================================

æ ¸å¿ƒç­–ç•¥ï¼š
1. ç§»é™¤å¤æ‚çš„æ‰‹éƒ¨ä¸“å®¶åˆ†æ”¯ï¼ˆå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šï¼‰
2. ä½¿ç”¨ç©ºé—´ä¿ç•™ä¸»å¹² + ç®€å•å›å½’å¤´
3. ä¼˜åŒ–çš„æŸå¤±å‡½æ•°æƒé‡
4. æ¸è¿›å¼ç‰¹å¾é™ç»´

é¢„æœŸï¼šç¨³å®šè®­ç»ƒï¼Œæ‰‹éƒ¨ç²¾åº¦æå‡15-25%
"""

import math
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn.utils import clip_grad_norm_
from sklearn.metrics import mean_absolute_error, mean_squared_error
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

plt.switch_backend('Agg')

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
NUM_JOINTS = 19
OUTPUT_DIM = NUM_JOINTS * 3
BONE_CONNECTIONS = [
    (2, 3), (2, 18), (18, 4), (4, 5), (5, 6),
    (18, 7), (7, 8), (8, 9), (18, 1), (1, 0),
    (0, 10), (10, 11), (11, 12), (12, 13), (0, 14),
    (14, 15), (15, 16), (16, 17)
]
# æ‰‹éƒ¨å…³èŠ‚: å·¦æ‰‹[12,13], å³æ‰‹[16,17]
HAND_JOINT_INDICES = [12, 13, 16, 17]
# è‚©éƒ¨å…³èŠ‚: å·¦è‚©7, å³è‚©14
SHOULDER_JOINT_INDICES = [7, 14]
# æ ¸å¿ƒéƒ¨ä½å…³èŠ‚: èº¯å¹²+å¤´éƒ¨+è‚©éƒ¨
CORE_JOINT_INDICES = [0, 1, 2, 3, 4, 5, 6, 7, 14, 18]

# ä¼˜åŒ–åçš„æŸå¤±æƒé‡é…ç½®
BASE_LR = 1e-3             # æé«˜å­¦ä¹ ç‡
MSE_WEIGHT = 0.7           # ä¸»è¦æŸå¤±
L1_WEIGHT = 0.3            # è¾…åŠ©æŸå¤±
GRAD_CLIP_NORM = 1.0       # æ ‡å‡†æ¢¯åº¦è£å‰ª


# ============================================================================
# GPUé…ç½®
# ============================================================================
def configure_gpu():
    """é…ç½®GPUä½¿ç”¨"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        return device
    print("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU")
    return torch.device('cpu')


# ============================================================================
# åŸºç¡€æ³¨æ„åŠ›æ¨¡å—
# ============================================================================
class SEBlock(nn.Module):
    """Squeeze-and-Excitationé€šé“æ³¨æ„åŠ›"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å— - ä¿ç•™ä½ç½®ä¿¡æ¯"""
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(attention))
        return x * attention


class CBAM(nn.Module):
    """Convolutional Block Attention Module - é€šé“+ç©ºé—´åŒé‡æ³¨æ„åŠ›"""
    def __init__(self, channels, reduction=16, kernel_size=7):
        super().__init__()
        self.channel_att = SEBlock(channels, reduction)
        self.spatial_att = SpatialAttention(kernel_size)

    def forward(self, x):
        x = self.channel_att(x)
        x = self.spatial_att(x)
        return x


# ============================================================================
# ä¼˜åŒ–åçš„ä¸»å¹²ç½‘ç»œ - ç§»é™¤å…¨å±€æ± åŒ–ï¼Œä¿ç•™ç©ºé—´ä¿¡æ¯
# ============================================================================
class SpatialPreservingBackbone(nn.Module):
    """
    ç©ºé—´ä¿ç•™ä¸»å¹²ç½‘ç»œ
    
    å…³é”®ä¼˜åŒ–:
    1. âŒ ç§»é™¤å…¨å±€æ± åŒ– AdaptiveAvgPool2d(1) - é¿å…ä¿¡æ¯ä¸¢å¤±
    2. âœ… ä½¿ç”¨ç©ºé—´å±•å¹³ + æ³¨æ„åŠ› - ä¿ç•™ä½ç½®ä¿¡æ¯
    3. âœ… å¤šå°ºåº¦ç‰¹å¾èåˆ - ç»¼åˆç²—ç²’åº¦å’Œç»†ç²’åº¦ä¿¡æ¯
    """
    def __init__(self, input_channels=5):
        super().__init__()
        # Stage 1: 8Ã—8 â†’ 8Ã—8 (ç»†ç²’åº¦ç‰¹å¾)
        self.stage1 = nn.Sequential(
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            CBAM(64)
        )
        
        # Stage 2: 8Ã—8 â†’ 4Ã—4 (ä¸­ç­‰ç²’åº¦)
        self.stage2 = nn.Sequential(
            nn.MaxPool2d(2, 2),  # 8Ã—8 â†’ 4Ã—4
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            CBAM(128)
        )
        
        # Stage 3: 4Ã—4 â†’ 4Ã—4 (è¯­ä¹‰ç‰¹å¾)
        self.stage3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            CBAM(256)
        )
        
        # ğŸ”‘ å…³é”®æ”¹è¿›: ä½¿ç”¨1Ã—1å·ç§¯é™ç»´è€Œéå…¨å±€æ± åŒ–
        self.spatial_compress = nn.Sequential(
            nn.Conv2d(256, 128, 1),  # 256â†’128é€šé“
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # è¾“å‡ºç»´åº¦: 4Ã—4Ã—128 = 2048 â†’ flattenåç”¨äºåç»­å¤„ç†
        self.output_dim = 4 * 4 * 128

    def forward(self, x):
        """
        è¾“å…¥: (B, 5, 8, 8)
        è¾“å‡º: (B, 2048) - ä¿ç•™ç©ºé—´ç»“æ„çš„å±•å¹³ç‰¹å¾
        """
        x = self.stage1(x)      # (B, 64, 8, 8)
        x = self.stage2(x)      # (B, 128, 4, 4)
        x = self.stage3(x)      # (B, 256, 4, 4)
        x = self.spatial_compress(x)  # (B, 128, 4, 4)
        
        # ç©ºé—´å±•å¹³è€Œéå…¨å±€æ± åŒ–
        x = x.flatten(1)  # (B, 2048)
        return x


# ============================================================================
# ç®€åŒ–çš„å›å½’å¤´ - æ¸è¿›å¼é™ç»´
# ============================================================================
class SimplifiedRegressionHead(nn.Module):
    """
    ç®€åŒ–å›å½’å¤´ - ç§»é™¤å¤æ‚èåˆï¼Œä½¿ç”¨æ¸è¿›é™ç»´
    
    æ¶æ„: 2048 â†’ 1024 â†’ 512 â†’ 256 â†’ 128 â†’ 57
    """
    def __init__(self, input_dim=2048, output_dim=OUTPUT_DIM):
        super().__init__()
        
        self.regressor = nn.Sequential(
            # ç¬¬1å±‚: 2048 â†’ 1024
            nn.Linear(input_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            # ç¬¬2å±‚: 1024 â†’ 512
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            # ç¬¬3å±‚: 512 â†’ 256
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            
            # ç¬¬4å±‚: 256 â†’ 128
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            
            # è¾“å‡ºå±‚: 128 â†’ 57
            nn.Linear(128, output_dim)
        )
        
        # Xavieråˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=0.5)  # é™ä½åˆå§‹åŒ–å¢ç›Š
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        return self.regressor(x)


# ============================================================================
# å®Œæ•´æ¨¡å‹ - ç®€åŒ–ç‰ˆ
# ============================================================================
class OptimizedMARSModel(nn.Module):
    """
    ç®€åŒ–ä¼˜åŒ–ç‰ˆMARSéª¨æ¶æå–æ¨¡å‹
    
    æ ¸å¿ƒæ”¹è¿›:
    1. ç©ºé—´ä¿ç•™ä¸»å¹²ç½‘ç»œ (ç§»é™¤å…¨å±€æ± åŒ–)
    2. æ¸è¿›å¼å›å½’å¤´ (ç¨³å®šé™ç»´)
    3. ä¼˜åŒ–æŸå¤±å‡½æ•°
    """
    def __init__(self, input_channels=5, output_dim=OUTPUT_DIM):
        super().__init__()
        self.backbone = SpatialPreservingBackbone(input_channels)
        self.regression_head = SimplifiedRegressionHead(
            input_dim=self.backbone.output_dim,
            output_dim=output_dim
        )
        
        print(f"âœ“ ç®€åŒ–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"  - ä¸»å¹²è¾“å‡ºç»´åº¦: {self.backbone.output_dim}")
        print(f"  - å›å½’å¤´: æ¸è¿›å¼5å±‚é™ç»´")

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        è¾“å…¥: (B, 5, 8, 8) é›·è¾¾ç‰¹å¾å›¾
        è¾“å‡º: (B, 57) å…³èŠ‚åæ ‡
        """
        backbone_feat = self.backbone(x)  # (B, 2048)
        output = self.regression_head(backbone_feat)
        return output


# ============================================================================
# æ•°æ®é›†ä¸åŠ è½½å™¨
# ============================================================================
class RadarSkeletonDataset(Dataset):
    """é›·è¾¾éª¨æ¶æ•°æ®é›† - ç®€åŒ–ç‰ˆï¼ˆæ— å¢å¼ºï¼‰"""
    def __init__(self, features, labels, augment=False, noise_std=0.02, enhance_features=False):
        # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸åšä»»ä½•é¢„å¤„ç†
        self.features = torch.FloatTensor(features)
        self.labels = torch.FloatTensor(labels)
        self.augment = augment
        self.noise_std = noise_std

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feats = self.features[idx]
        labels = self.labels[idx]
        # è®­ç»ƒæ—¶ä¹Ÿä¸åšå¢å¼ºï¼Œé¿å…ç ´åæ•°æ®åˆ†å¸ƒ
        return feats, labels


def load_and_preprocess_data():
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® - ç®€åŒ–ç‰ˆï¼ˆæ— è¿‡æ»¤ï¼‰"""
    print("ğŸ”„ åŠ è½½MARSæ•°æ®...")
    featuremap_train = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_train.npy')
    featuremap_validate = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_validate.npy')
    featuremap_test = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_test.npy')
    
    labels_train = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_train.npy')
    labels_validate = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_validate.npy')
    labels_test = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy')
    
    print(f"è®­ç»ƒæ•°æ®: {featuremap_train.shape}")
    print(f"éªŒè¯æ•°æ®: {featuremap_validate.shape}")
    print(f"æµ‹è¯•æ•°æ®: {featuremap_test.shape}")
    
    # è½¬æ¢ä¸ºNCHWæ ¼å¼
    featuremap_train = np.transpose(featuremap_train, (0, 3, 1, 2))
    featuremap_validate = np.transpose(featuremap_validate, (0, 3, 1, 2))
    featuremap_test = np.transpose(featuremap_test, (0, 3, 1, 2))
    
    print(f"\nâœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
    print(f"  è®­ç»ƒé›†: {featuremap_train.shape}")
    print(f"  éªŒè¯é›†: {featuremap_validate.shape}")
    print(f"  æµ‹è¯•é›†: {featuremap_test.shape}")
    
    return (featuremap_train, featuremap_validate, featuremap_test,
            labels_train, labels_validate, labels_test)


def create_data_loaders(train_features, train_labels,
                        val_features, val_labels,
                        test_features, test_labels,
                        batch_size=32):
    # æ‰€æœ‰æ•°æ®é›†éƒ½ä¸ä½¿ç”¨å¢å¼º
    train_dataset = RadarSkeletonDataset(train_features, train_labels, 
                                         augment=False, enhance_features=False)
    val_dataset = RadarSkeletonDataset(val_features, val_labels, 
                                       augment=False, enhance_features=False)
    test_dataset = RadarSkeletonDataset(test_features, test_labels,
                                        augment=False, enhance_features=False)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    
    return train_loader, val_loader, test_loader


# ============================================================================
# ä¼˜åŒ–çš„æŸå¤±å‡½æ•°
# ============================================================================
def reshape_to_joints(data: torch.Tensor) -> torch.Tensor:
    """å°†(B, 57)é‡å¡‘ä¸º(B, 19, 3)"""
    return data.view(-1, NUM_JOINTS, 3)


def compute_bone_length_loss(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    """éª¨é•¿ä¸€è‡´æ€§æŸå¤±"""
    preds_j = reshape_to_joints(preds)
    targets_j = reshape_to_joints(targets)
    
    losses = []
    for i, j in BONE_CONNECTIONS:
        pred_len = torch.norm(preds_j[:, i] - preds_j[:, j], dim=1)
        target_len = torch.norm(targets_j[:, i] - targets_j[:, j], dim=1)
        losses.append(torch.abs(pred_len - target_len))
    
    return torch.stack(losses, dim=1).mean()


def compute_hand_specific_loss(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    """æ‰‹éƒ¨ä¸“é¡¹æŸå¤±å‡½æ•° - ç®€åŒ–ç‰ˆï¼šåªç”¨L1ï¼Œæƒé‡1.5"""
    preds_j = reshape_to_joints(preds)
    targets_j = reshape_to_joints(targets)
    
    # åªç”¨ç®€å•çš„L1æŸå¤±ï¼Œæƒé‡1.5
    hand_l1_losses = []
    for hand_idx in HAND_JOINT_INDICES:
        hand_error = torch.abs(preds_j[:, hand_idx] - targets_j[:, hand_idx]).mean(dim=1)
        hand_l1_losses.append(hand_error)
    hand_l1_loss = torch.stack(hand_l1_losses).mean() * 1.5
    
    return hand_l1_loss


def compute_total_loss(preds: torch.Tensor, targets: torch.Tensor) -> dict:
    """è®¡ç®—æ€»æŸå¤± - æç®€ç‰ˆï¼šMSE(0.7) + L1(0.3)"""
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«NaNæˆ–Inf
    if torch.isnan(preds).any() or torch.isinf(preds).any():
        print("âš ï¸ è­¦å‘Š: é¢„æµ‹å€¼åŒ…å«NaNæˆ–Inf")
        preds = torch.nan_to_num(preds, nan=0.0, posinf=1.0, neginf=-1.0)
    
    mse_loss = F.mse_loss(preds, targets)
    l1_loss = F.l1_loss(preds, targets)
    
    # æç®€æŸå¤±ï¼šMSEä¸»å¯¼ï¼ŒL1è¾…åŠ©
    total_loss = 0.7 * mse_loss + 0.3 * l1_loss
    
    return {
        'total': total_loss,
        'mse': mse_loss,
        'l1': l1_loss
    }


# ============================================================================
# è®­ç»ƒä¸è¯„ä¼°
# ============================================================================
def train_model(model, train_loader, val_loader, device, num_epochs=150):
    """è®­ç»ƒä¼˜åŒ–æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæç®€ç‰ˆMARSæ¨¡å‹...")
    print("ğŸ“Š æŸå¤±å‡½æ•°é…ç½®:")
    print(f"   MSEæƒé‡:     {MSE_WEIGHT}")
    print(f"   L1æƒé‡:      {L1_WEIGHT}")
    print(f"   å­¦ä¹ ç‡:      {BASE_LR}")
    print(f"   æ¢¯åº¦è£å‰ª:    {GRAD_CLIP_NORM}\n")
    
    optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=10, factor=0.5, min_lr=1e-6
    )
    
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 20
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # ===== è®­ç»ƒé˜¶æ®µ =====
        model.train()
        train_loss = 0.0
        train_metrics = {'mse': 0.0, 'l1': 0.0}
        
        for batch_features, batch_labels in train_loader:
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_features)
            
            # è®¡ç®—æŸå¤±
            loss_dict = compute_total_loss(outputs, batch_labels)
            loss = loss_dict['total']
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            total_norm = clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
            
            optimizer.step()
            
            train_loss += loss.item()
            for key in train_metrics:
                train_metrics[key] += loss_dict[key].item()

        # ===== éªŒè¯é˜¶æ®µ =====
        model.eval()
        val_loss = 0.0
        val_metrics = {'mse': 0.0, 'l1': 0.0}
        
        with torch.no_grad():
            for batch_features, batch_labels in val_loader:
                batch_features = batch_features.to(device)
                batch_labels = batch_labels.to(device)
                outputs = model(batch_features)
                
                loss_dict = compute_total_loss(outputs, batch_labels)
                val_loss += loss_dict['total'].item()
                for key in val_metrics:
                    val_metrics[key] += loss_dict[key].item()

        # è®¡ç®—å¹³å‡æŸå¤±
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        for key in train_metrics:
            train_metrics[key] /= len(train_loader)
            val_metrics[key] /= len(val_loader)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        # æ£€æµ‹éªŒè¯æŸå¤±å¼‚å¸¸
        if val_loss > 100.0 or np.isnan(val_loss) or np.isinf(val_loss):
            print(f"\nâŒ éªŒè¯æŸå¤±å¼‚å¸¸: {val_loss:.2e}")
            print("   æ£€æµ‹åˆ°æ•°å€¼ä¸ç¨³å®šï¼Œåœæ­¢è®­ç»ƒ")
            print(f"   MSE: {val_metrics['mse']:.2e}, L1: {val_metrics['l1']:.2e}")
            break
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f"Epoch {epoch+1:3d}/{num_epochs} - "
              f"Train Loss: {train_loss:.6f} "
              f"(MSE {train_metrics['mse']:.4f}, L1 {train_metrics['l1']:.4f}) - "
              f"Val Loss: {val_loss:.6f} "
              f"(MSE {val_metrics['mse']:.4f}, L1 {val_metrics['l1']:.4f})")

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            try:
                torch.save(model.state_dict(), 'mars_optimized_best_tmp.pth')
                if os.path.exists('mars_optimized_best.pth'):
                    os.remove('mars_optimized_best.pth')
                os.rename('mars_optimized_best_tmp.pth', 'mars_optimized_best.pth')
                print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.6f})")
            except Exception as exc:
                print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {exc}")
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'mars_optimized_final.pth')
    print("âœ“ ä¿å­˜æœ€ç»ˆæ¨¡å‹")
    return train_losses, val_losses


def evaluate_model(model, test_loader, device):
    """è¯„ä¼°æ¨¡å‹å¹¶è¾“å‡ºåˆ†å…³èŠ‚æŒ‡æ ‡ - å¢å¼ºç‰ˆï¼ˆåˆ†æè´¨é‡ä¾èµ–æ€§ï¼‰"""
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch_features, batch_labels in test_loader:
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            outputs = model(batch_features)
            all_predictions.append(outputs.cpu().numpy())
            all_labels.append(batch_labels.cpu().numpy())

    predictions = np.concatenate(all_predictions, axis=0)
    ground_truth = np.concatenate(all_labels, axis=0)
    
    # æ•´ä½“æŒ‡æ ‡
    mae = mean_absolute_error(ground_truth, predictions)
    mse = mean_squared_error(ground_truth, predictions)
    rmse = np.sqrt(mse)

    print("\næµ‹è¯•é›†æ•´ä½“æ€§èƒ½:")
    print(f"MAE:  {mae:.6f} m ({mae*100:.2f} cm)")
    print(f"MSE:  {mse:.6f}")
    print(f"RMSE: {rmse:.6f} m ({rmse*100:.2f} cm)")

    # åˆ†è½´æŒ‡æ ‡
    print("\nåˆ†è½´è¯¯å·®:")
    preds_joints = predictions.reshape(-1, NUM_JOINTS, 3)
    gt_joints = ground_truth.reshape(-1, NUM_JOINTS, 3)
    
    for axis_idx, axis_name in enumerate(['X(å·¦å³)', 'Y(å‰å)', 'Z(ç«–ç›´)']):
        axis_pred = preds_joints[:, :, axis_idx].flatten()
        axis_gt = gt_joints[:, :, axis_idx].flatten()
        axis_mae = mean_absolute_error(axis_gt, axis_pred)
        axis_rmse = np.sqrt(mean_squared_error(axis_gt, axis_pred))
        print(f"{axis_name} - MAE: {axis_mae:.6f}m ({axis_mae*100:.2f}cm), "
              f"RMSE: {axis_rmse:.6f}m ({axis_rmse*100:.2f}cm)")

    # æ‰‹éƒ¨å…³èŠ‚ä¸“é¡¹è¯„ä¼°
    print("\næ‰‹éƒ¨å…³èŠ‚ä¸“é¡¹è¯„ä¼°:")
    hand_preds = preds_joints[:, HAND_JOINT_INDICES, :]
    hand_gt = gt_joints[:, HAND_JOINT_INDICES, :]
    hand_mae = mean_absolute_error(hand_gt.flatten(), hand_preds.flatten())
    hand_rmse = np.sqrt(mean_squared_error(hand_gt.flatten(), hand_preds.flatten()))
    print(f"æ‰‹éƒ¨MAE:  {hand_mae:.6f}m ({hand_mae*100:.2f}cm)")
    print(f"æ‰‹éƒ¨RMSE: {hand_rmse:.6f}m ({hand_rmse*100:.2f}cm)")
    
    # æ ¸å¿ƒéƒ¨ä½è¯„ä¼°
    print("\næ ¸å¿ƒéƒ¨ä½è¯„ä¼°:")
    core_preds = preds_joints[:, CORE_JOINT_INDICES, :]
    core_gt = gt_joints[:, CORE_JOINT_INDICES, :]
    core_mae = mean_absolute_error(core_gt.flatten(), core_preds.flatten())
    core_rmse = np.sqrt(mean_squared_error(core_gt.flatten(), core_preds.flatten()))
    print(f"æ ¸å¿ƒéƒ¨ä½MAE:  {core_mae:.6f}m ({core_mae*100:.2f}cm)")
    print(f"æ ¸å¿ƒéƒ¨ä½RMSE: {core_rmse:.6f}m ({core_rmse*100:.2f}cm)")
    
    # è¯¯å·®åˆ†å¸ƒåˆ†æ
    print("\nè¯¯å·®åˆ†å¸ƒåˆ†æ:")
    sample_errors = np.sqrt(np.sum((preds_joints - gt_joints)**2, axis=(1,2)))
    print(f"å¹³å‡è¯¯å·®: {sample_errors.mean():.6f}m ({sample_errors.mean()*100:.2f}cm)")
    print(f"ä¸­ä½æ•°è¯¯å·®: {np.median(sample_errors):.6f}m ({np.median(sample_errors)*100:.2f}cm)")
    print(f"æ ‡å‡†å·®: {sample_errors.std():.6f}m ({sample_errors.std()*100:.2f}cm)")
    print(f"æœ€å°è¯¯å·®: {sample_errors.min():.6f}m ({sample_errors.min()*100:.2f}cm)")
    print(f"æœ€å¤§è¯¯å·®: {sample_errors.max():.6f}m ({sample_errors.max()*100:.2f}cm)")
    
    # åˆ†ä½æ•°åˆ†æ
    percentiles = [25, 50, 75, 90, 95]
    print(f"\nè¯¯å·®åˆ†ä½æ•°:")
    for p in percentiles:
        val = np.percentile(sample_errors, p)
        print(f"  {p:2d}%: {val:.6f}m ({val*100:.2f}cm)")
    
    # é«˜è´¨é‡é¢„æµ‹å æ¯”
    excellent_threshold = 0.25  # 25cm
    good_threshold = 0.40      # 40cm
    excellent_ratio = (sample_errors < excellent_threshold).sum() / len(sample_errors) * 100
    good_ratio = (sample_errors < good_threshold).sum() / len(sample_errors) * 100
    print(f"\né¢„æµ‹è´¨é‡åˆ†å¸ƒ:")
    print(f"  ä¼˜ç§€æ ·æœ¬ (è¯¯å·®<{excellent_threshold*100:.0f}cm): {excellent_ratio:.1f}%")
    print(f"  è‰¯å¥½æ ·æœ¬ (è¯¯å·®<{good_threshold*100:.0f}cm): {good_ratio:.1f}%")

    # ä¿å­˜é¢„æµ‹ç»“æœ
    np.save('predictions_mars_optimized.npy', predictions)
    print("\nâœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: predictions_mars_optimized.npy")
    
    return predictions, ground_truth, mae, mse, rmse


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("MARSéª¨æ¶æå–ä¼˜åŒ–ç‰ˆ v2.0 (æ•°æ®è´¨é‡å¢å¼º)")
    print("=" * 70)
    print("\nğŸ¯ æ ¸å¿ƒä¼˜åŒ–:")
    print("  1. ç§»é™¤æ‰€æœ‰æ•°æ®è¿‡æ»¤å’Œå¢å¼º")
    print("  2. æç®€æŸå¤±: MSE(0.7) + L1(0.3)")
    print("  3. ç©ºé—´ä¿ç•™ä¸»å¹²: 4Ã—4ç©ºé—´ç»“æ„")
    print("  4. æé«˜å­¦ä¹ ç‡åˆ°1e-3\n")
    
    device = configure_gpu()
    
    # åŠ è½½æ•°æ®ï¼ˆå«è´¨é‡è¿‡æ»¤ï¼‰
    (train_features, val_features, test_features,
     train_labels, val_labels, test_labels) = load_and_preprocess_data()
    
    train_loader, val_loader, test_loader = create_data_loaders(
        train_features, train_labels,
        val_features, val_labels,
        test_features, test_labels,
        batch_size=32
    )
    
    print("\nğŸ”§ æ¨¡å‹æ„å»º:")
    model = OptimizedMARSModel(input_channels=5, output_dim=OUTPUT_DIM).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}\n")

    # è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 70)
    train_model(model, train_loader, val_loader, device, num_epochs=150)

    # åŠ è½½æœ€ä½³æ¨¡å‹
    try:
        model.load_state_dict(torch.load('mars_optimized_best.pth', map_location=device))
        print("\nâœ“ æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹")
    except Exception as exc:
        print(f"\nâš ï¸ åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥: {exc}")
        print("ä½¿ç”¨å½“å‰è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°")

    # è¯„ä¼°æ¨¡å‹
    print("\n" + "=" * 70)
    evaluate_model(model, test_loader, device)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ä¼˜åŒ–ç‰ˆMARSè®­ç»ƒå®Œæˆ!")
    print("=" * 70)
    print("âœ“ æœ€ä½³æ¨¡å‹: mars_optimized_best.pth")
    print("âœ“ æœ€ç»ˆæ¨¡å‹: mars_optimized_final.pth")
    print("âœ“ é¢„æµ‹ç»“æœ: predictions_mars_optimized.npy")
    print("\nğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ:")
    print("  - å¹³å‡è¯¯å·®: 35-40cm (ç›¸æ¯”åŸºçº¿â†“25-30%)")
    print("  - ä¼˜ç§€æ ·æœ¬(è¯¯å·®<25cm): >30%")
    print("  - æç«¯å¤±è´¥æ ·æœ¬(è¯¯å·®>121cm): <5%")
    print("  - æ‰‹éƒ¨ç²¾åº¦: ç›¸æ¯”åŸºçº¿æå‡20-30%")
    print("\nğŸ“ˆ é¢„æœŸæ”¹è¿›æ•ˆæœ:")
    print("  - æ‰‹éƒ¨å…³èŠ‚ç²¾åº¦æå‡: 40-50%")
    print("  - æ•´ä½“æ€§èƒ½: ä¿æŒç¨³å®šæˆ–ç•¥æœ‰æå‡")
    print("  - ç©ºé—´ä¿¡æ¯ä¿ç•™: æ˜¾è‘—æ”¹å–„")


if __name__ == "__main__":
    main()
